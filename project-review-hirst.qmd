---
title: Project Review Template 
author: Cora Hirst
date: date-modified
format: 
  html:
    toc: true
    toc-depth: 3
    number-sections: true
---

# Overview

Title of project: "Geographic Spread of Seasonal Flu"

Name of project author(s): Clarke Miller

Name of project reviewer: Cora Hirst

  
# Specific project content evaluation
Evaluate the different parts of the project by filling in the sections below.


## Background, Context and Motivation
How well is the context of the project described? Is a comprehensive background, including summary of previous/related work given? Is the project well placed into the context of existing work (including proper referencing of existing work). Is it clear why the project was undertaken and what new information it hopes to provide?

### Feedback and Comments

The abstract provides a nice overview of the dynamics of seasonal flu variation and mechanisms of escape mutation. I appreciated this in light of the project - tracing flu strain movement geographically - because it provided nice motivation: where do potentially problematic variants occur? Are they always generated in the same place? Can we blame recombination, waning immunity, seasonality...?

It would be nice to see this in the introduction - but I did appreciate the public-health motivation.

### Summary assessment 

* strong contextualization and motivation


## Question description
How well and clear are the question(s)/hypotheses the project aims to address described? Is it clear how the questions relate to the data?

### Feedback and Comments

The questions were well defined and related nicely to the presetnation of the data (figure) in the previous section. Well done!

I will say, it felt a bit ambitious. It may have been nice to choose two (or even the last one, alone) and really dig into it carefully, especially in light of a lot of the literature out there! One of the classic papers on influenza seasonality is Jonathon Dushoff and crew's 2004 PNAS "Dynamical resonance can account for seasonality of influenza epidemics" - there are lots of good papers cited (and who cite this one!)

### Summary assessment

* question/hypotheses fully clear

## Data description
How well is the data overall described? Is the source provided? Is a codebook or other meta-information available that makes it clear what the data is? 

### Feedback and Comments

This is a strength! You provide access to lots of different repositories of flu data, and clearly indicate that from which you draw the dataset you conduct your analysis on. 


### Summary assessment
* source and overall structure of data poorly explained
* source and overall structure of data somewhat explained
* source and overall structure of data well explained


## Data wrangling and exploratory analysis
How well is the data cleaned/processed and explored? Are all steps reasonable and well explained? Are alternatives discussed and considered? Are meaningful exploratory results shown (e.g. in the supplementary materials)?

### Feedback and Comments

I would have liked to know how much missing data was present and on what criteria you decided to remove observations while cleaning! Also, noting where the data is saved would be helpful, either in the cleaning document itself or in the README file. 

I also wasn't sure which files to run and in what order to reproduce your figures. That said, the figures themselves are very nice - a appreciate the inclusion of figure for total global cases initially, and filtering through which regions saw what burden, when.

### Summary assessment

* some weaknesses in wrangling and exploratory component


## Appropriateness of Analysis
Were the analysis methods appropriate for the data? Was the analysis done properly? Were different components of the analysis (e.g. performance measure, variable selection, data pre-processing, model evaluation) done in the best way possible and explained well?

### Feedback and Comments

WRITE YOUR FEEDBACK HERE

The initial analysis was the perfect first step to take - how can we associate all of our variables with the outcome, and how good is our model at making predictions? I would have liked to see some sort of cross-validation to look for overfitting here, but this was definitely the right train of thought. 

I also thought he normalization top the number of reported cases was a thoughtful and necessary addition, so bravo!

The random forest model was a nice way to go, though if you're looking to get any mechanistic insight on disease spread and not just predictive capacity, I'm not sure the model will be as helpful. A linear regression which idenitifies the most significant contirbutors (either by adding one at a time and testing the senesitivity of the output to each predictor, or taking one away at a time and assessing the same - ideally, both!) may have been a helpful step after the full multiple regression model. 

### Summary assessment

* strong and reasonable analysis

## Presentation
How well are results presented? Are tables and figures easy to read and understand? Are the main figures/tables publication level quality? 

### Feedback and Comments

You presented the right figures to support your conclusions! I would've just liked to see axis labels and either better figure titles, or the removal of titles altogether and deferrment to the caption for explanation of what is being shown. 

### Summary assessment

* results are presented ok, with room for improvement


## Discussion/Conclusions
Are the study findings properly discussed? Are strengths and limitations acknowledged? Are findings interpreted properly?

### Feedback and Comments

I'm not sure that the conclusion that the data is not amiable to a machine learning model is *compeltely* supported by your analysis, as there are quite a few other models that may make more sense to use here! Also, tuning the random forest model may have helped its performance, as well, and including CV (especially considering how much data you have at your fingertips!) may have helped you choose the better model and determned some significance. 

I appreciated that you aknowledge this in the discussion, and that you identified additional features that could improve the breadth of conclusions you could draw from the analysis. That's a wonderful way to start thinking ahead to new study designs!
 - 
### Summary assessment

* strong, complete and clear discussion


## Further comments

Overall I enjoyed the exercise of trying to predict flu emergence with RF models - especially because it helped familiarize me with some of the data sets out there that I could use in my own research! (selfish, perhaps, but regardless, thanks!)

# Overall project content evaluation
Evaluate overall features of the project  by filling in the sections below.


## Structure
Is the project well structured? Are files in well labeled folders? Do files have reasonable names? Are all "junk" files not needed for analysis/reproduction removed? By just looking at files and folders, can you get an idea of how things fit together?

### Feedback and Comments

The structure could use a bit of improvement - some of the processing files could be deleted, and a clearer picture of which files need to be run and when would be helpful to fully reproduce your results! 

### Summary assessment

* mostly clear, but some confusing parts (e.g. useless files, things in the wrong folders)



## Documentation 
How well is the project documented? Are you able to understand each step of the whole analysis, each decision that was made, and each line of code? Is enough information provided as comments in code or as part of Rmd files? 

### Feedback and Comments

Comments on what to run, when, and where outputs are stored would be helpful, and also an aknowedgement of the rational for the choices made during analysis would really improve the reproducibility of the project :)

### Summary assessment

* decently documented with some gaps




## Reproducibility
Are all results fully reproducible? Is documentation provided which clearly explains how to reproduce things, and does it work without the need for any manual intervention? Are you able to re-run the whole analysis without having to do manual interventions/edits?

### Feedback and Comments

All files that I could find that were necessary to run to produce the outputs relecant to the manuscript were reproducible. Bravo!


### Summary assessment

* fully reproducible without issues


## Thoroughness
How thorough was the overall study? Were alternatives (e.g. different ways of processing the data or different models) considered? Were alternatives discussed? Were the questions/hypotheses fully and thoroughly addressed?

### Feedback and Comments

Another strength of yours is identifying and discussing alternatives and improvements that could be made to your models. Identifying some other models that could have been used would be helpful (I didn't do this, either!)

### Summary assessment

* strong level of thoroughness





